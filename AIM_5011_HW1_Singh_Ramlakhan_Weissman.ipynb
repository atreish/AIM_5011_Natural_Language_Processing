{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Atreish Ramlakhan, Aishwarya Singh, Nosson Weissman\n",
    "AIM 5011: Natural Language Processing\n",
    "Fall 2021\n",
    "HW #1: Spam and Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import makedirs, path, remove, rename, rmdir\n",
    "from tarfile import open as open_tar\n",
    "from shutil import rmtree\n",
    "from urllib import request, parse\n",
    "from glob import glob\n",
    "from os import path\n",
    "from re import sub\n",
    "from email import message_from_file\n",
    "from glob import glob\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``This code block downloads the folder with spam and non-spam emails from the online source and \n",
    "creates local directories where this python notebook is stored``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_corpus(dataset_dir: str = 'data'):\n",
    "    base_url = 'https://spamassassin.apache.org'\n",
    "    corpus_path = 'old/publiccorpus'\n",
    "    files = {\n",
    "        '20021010_easy_ham.tar.bz2': 'ham',\n",
    "        '20021010_hard_ham.tar.bz2': 'ham',\n",
    "        '20021010_spam.tar.bz2': 'spam',\n",
    "        '20030228_easy_ham.tar.bz2': 'ham',\n",
    "        '20030228_easy_ham_2.tar.bz2': 'ham',\n",
    "        '20030228_hard_ham.tar.bz2': 'ham',\n",
    "        '20030228_spam.tar.bz2': 'spam',\n",
    "        '20030228_spam_2.tar.bz2': 'spam',\n",
    "        '20050311_spam_2.tar.bz2': 'spam' }\n",
    "    \n",
    "    #creates the folders: downloads, ham and spam\n",
    "    downloads_dir = path.join(dataset_dir, 'downloads')\n",
    "    ham_dir = path.join(dataset_dir, 'ham')\n",
    "    spam_dir = path.join(dataset_dir, 'spam')\n",
    "\n",
    "    makedirs(downloads_dir, exist_ok=True)\n",
    "    makedirs(ham_dir, exist_ok=True)\n",
    "    makedirs(spam_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    for file, spam_or_ham in files.items():\n",
    "        # download files from URL of each specific .bz2 file \n",
    "        url = parse.urljoin(base_url, f'{corpus_path}/{file}')\n",
    "        tar_filename = path.join(downloads_dir, file)\n",
    "        request.urlretrieve(url, tar_filename)\n",
    "        \n",
    "        #list e-mails in the compressed .bz2 file\n",
    "        emails = []\n",
    "        with open_tar(tar_filename) as tar:\n",
    "            tar.extractall(path=downloads_dir)\n",
    "            for tarinfo in tar:\n",
    "                if len(tarinfo.name.split('/')) > 1:\n",
    "                    emails.append(tarinfo.name)\n",
    "        \n",
    "        # move e-mails to ham or spam directory\n",
    "        for email in emails:\n",
    "            directory, filename = email.split('/')\n",
    "            directory = path.join(downloads_dir, directory)\n",
    "            \n",
    "            if not path.exists(path.join(dataset_dir, spam_or_ham, filename)):\n",
    "                rename(path.join(directory, filename),\n",
    "                   path.join(dataset_dir, spam_or_ham, filename))\n",
    "                \n",
    "        rmtree(directory)\n",
    "\n",
    "download_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Non-Spam E-mails: 6952\n",
      "\n",
      "Number of Spam E-mails: 2399\n"
     ]
    }
   ],
   "source": [
    "#How many e-mails are classified in our dataset as either Spam or not Spam?\n",
    "ham_dir = path.join('data', 'ham')\n",
    "spam_dir = path.join('data', 'spam')\n",
    "\n",
    "print('Number of Non-Spam E-mails:', len(glob(f'{ham_dir}/*')))  \n",
    "print('\\nNumber of Spam E-mails:', len(glob(f'{spam_dir}/*')))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```This class retreives a clean string for each e-mail with a subject line text and body text```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmail:\n",
    "    def __init__(self, subject: str, body: str):\n",
    "        self.subject = subject\n",
    "        self.body = body\n",
    "    \n",
    "    #as long as its not a letter, make it blank\n",
    "    @property\n",
    "    def clean(self):\n",
    "        sanitizer = '[^A-Za-z]+' #non-letters\n",
    "        clean = sub(sanitizer, ' ', f'{self.subject} {self.body}') #replace non-letters with space\n",
    "        clean = clean.lower()\n",
    "        return sub('\\s+', ' ', clean) \n",
    "    \n",
    "    #this function classifies the subject and body of e-mail by first new line \\n. Returns email as a string\n",
    "    def __str__(self):\n",
    "        subject = f'subject: {self.subject}'\n",
    "        body_first_line = self.body.split('\\n')[0]\n",
    "        body = f'body: {body_first_line}...'\n",
    "        return f'{subject}\\n{body}' #output is subject and body Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```This class treats the entire folder of emails as if it were a list, as an iterable object then \n",
    "applies a parsing function to each list element```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailIterator:\n",
    " \n",
    "    #This function pulls every file from whatever our imput directory, positions it's iteration 0  \n",
    "    def __init__(self, directory: str):\n",
    "        self._files = glob(f'{directory}/*')\n",
    "        self._pos = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._pos = -1\n",
    "        return self\n",
    "    \n",
    "    #if the position is not at the end of the list of emails, we apply parse_emails and returns the output\n",
    "    def __next__(self):\n",
    "        if self._pos < len(self._files) - 1:\n",
    "            self._pos += 1\n",
    "            return self.parse_email(self._files[self._pos])\n",
    "        raise StopIteration()\n",
    "\n",
    "    #This function defines each file as a SimpleEmail class object with subject and body\n",
    "    @staticmethod\n",
    "    def parse_email(filename: str) -> SimpleEmail:\n",
    "        with open(filename, encoding='utf-8', errors='replace') as fp:\n",
    "            message = message_from_file(fp)\n",
    "        \n",
    "        subject = None\n",
    "        for item in message.raw_items():  \n",
    "            if item[0] == 'Subject':\n",
    "                subject = item[1] \n",
    "        \n",
    "        if message.is_multipart(): #is this multipart? if so add to body list of sub email message objects\n",
    "            body = []\n",
    "            for b in message.get_payload():  #add each part to the body and iterate through it \n",
    "                body.append(str(b))\n",
    "            body = '\\n'.join(body)\n",
    "        else:\n",
    "            body = message.get_payload() #just one part of the body the payload is one part\n",
    "        \n",
    "        return SimpleEmail(subject, body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Apply the EmailIterator class to our Spam and Non-Spam e-mail folders and then transform it into\n",
    "an array of cleaned E-mail objects with subject and body```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the class to each class of e-mails\n",
    "ham_emails = EmailIterator('data/ham')\n",
    "spam_emails = EmailIterator('data/spam')\n",
    "\n",
    "#Numpy arrays of our Ham and Spam e-mails. Our data!\n",
    "hams = np.array([email.clean for email in ham_emails])\n",
    "spams = np.array([email.clean for email in spam_emails])\n",
    "\n",
    "#purge memory we are not using\n",
    "del ham_emails\n",
    "del spam_emails\n",
    "gc.collect() \n",
    "\n",
    "#Train test split 80, 20 for our classifier by balancing the % of training and testing for each class\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\n",
    "emails = np.concatenate((hams, spams))\n",
    "labels = np.concatenate((np.zeros(hams.size), np.ones(spams.size)))\n",
    "\n",
    "#clears memory that doesnt get used\n",
    "del hams\n",
    "del spams\n",
    "gc.collect()\n",
    "\n",
    "#Ensuring labels are aligned with emails, each label is correctly associated with each email\n",
    "for train_index, test_index in split.split(emails, labels):\n",
    "    emails_train, labels_train = \\\n",
    "        emails[train_index], labels[train_index]\n",
    "    emails_test, labels_test = \\\n",
    "        emails[test_index], labels[test_index]\n",
    "    \n",
    "#Importing word dictionary to create term document matrix, counting occurances of each word\n",
    "dictionary = defaultdict(int)\n",
    "for email in emails_train:\n",
    "    for word in email.split(' '):\n",
    "        dictionary[word] += 1\n",
    "\n",
    "#Selecting top 1000 most occured words, and lists its occurrances in each email\n",
    "top = 1000\n",
    "descending_dictionary = sorted(dictionary.items(), key=lambda v: v[1], reverse=True)\n",
    "dictionary = [word for (word, occur) in descending_dictionary if len(word) > 1][:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``The following function creates a term document matrix covering the top 1000 most occuring words in the\n",
    "corpus (document) either one-hot encoding format if binary is True or frequency encoding if binary is False``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_email(email: SimpleEmail, dictionary_: list, binary: bool = False) -> np.array:\n",
    "    encoded = np.zeros(dictionary_.size)\n",
    "    words = email.split(' ')\n",
    "    \n",
    "    for word in words:\n",
    "        index = np.where(dictionary_ == word)[0]\n",
    "        if index.size == 1:  #we ignore unknown words if they do not appear in top 1000\n",
    "            if binary: \n",
    "                encoded[index[0]] = 1 #set to 1 if the word exists\n",
    "            else:\n",
    "                encoded[index[0]] += 1 #otherwise add to the count i.e. frequency of the word\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Note: the Term Document matrix we ouput will have each e-mail in our data is a row \n",
    "and there are 1000 columns (for each word)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = np.array(dictionary)  \n",
    "\n",
    "#Every email is encoded as a different term document matrix row\n",
    "_encode_email = partial(encode_email, dictionary_=dictionary)\n",
    "\n",
    "#Train as numpy arrays\n",
    "encoded_train = np.array(list(map(_encode_email, emails_train))) #entire term document matrix with 80%\n",
    "encoded_test = np.array(list(map(_encode_email, emails_test))) #testing the term document matrix with 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model: 0.9637700534759358\n",
      "Precision of Model: 0.9629213483146067\n",
      "Recall of Model: 0.8931735278791036\n",
      "F1 Score of Model: 0.9267369559340363\n"
     ]
    }
   ],
   "source": [
    "#The classifer we use is K-Nearest Neighbor from Sci-Kit Learn\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "#Testing with 5 cross validation folds\n",
    "labels_pred = cross_val_predict(knn_clf, encoded_train,labels_train, cv=5)\n",
    "print('Accuracy of Model:', accuracy_score(labels_train, labels_pred))\n",
    "print('Precision of Model:', precision_score(labels_train, labels_pred))\n",
    "print('Recall of Model:', recall_score(labels_train, labels_pred))\n",
    "print('F1 Score of Model:', f1_score(labels_train, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model: 0.9643048128342246\n",
      "Precision of Model: 0.9619686800894854\n",
      "Recall of Model: 0.8963001563314226\n",
      "F1 Score of Model: 0.9279741030482871\n"
     ]
    }
   ],
   "source": [
    "#Testing with 10 cross validation folds\n",
    "labels_pred = cross_val_predict(knn_clf, encoded_train,labels_train, cv=10)\n",
    "print('Accuracy of Model:', accuracy_score(labels_train, labels_pred))\n",
    "print('Precision of Model:', precision_score(labels_train, labels_pred))\n",
    "print('Recall of Model:', recall_score(labels_train, labels_pred))\n",
    "print('F1 Score of Model:', f1_score(labels_train, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model: 0.9649732620320856\n",
      "Precision of Model: 0.9605336297943302\n",
      "Recall of Model: 0.9004689942678479\n",
      "F1 Score of Model: 0.9295320064550834\n"
     ]
    }
   ],
   "source": [
    "#Testing with 15 cross validation folds\n",
    "labels_pred = cross_val_predict(knn_clf, encoded_train,labels_train, cv=15)\n",
    "print('Accuracy of Model:', accuracy_score(labels_train, labels_pred))\n",
    "print('Precision of Model:', precision_score(labels_train, labels_pred))\n",
    "print('Recall of Model:', recall_score(labels_train, labels_pred))\n",
    "print('F1 Score of Model:', f1_score(labels_train, labels_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``References``\n",
    "\n",
    "https://colab.research.google.com/github/PseudoCodeNerd/blog/blob/master/_notebooks/2019-10-19-spamClassifier-Oreilly-homework-chapter3.ipynb#scrollTo=dePbzCrARDID\n",
    "\n",
    "https://www.youtube.com/watch?v=8rXD5-xhemo&t=1103s\n",
    "\n",
    "https://medium.com/@thiagolcmelo/train-you-own-spam-detector-57725e8e81c0\n",
    "\n",
    "https://towardsdatascience.com/spam-or-ham-introduction-to-natural-language-processing-part-2-a0093185aebd\n",
    "\n",
    "https://github.com/happilyeverafter95/Medium/blob/master/spam_or_ham.py\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
